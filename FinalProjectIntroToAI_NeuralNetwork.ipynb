{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d81e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time, random, numpy as np, torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data\"):\n",
    "    with zipfile.ZipFile(\"data.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "NUM_TRAINING = 1000\n",
    "NUM_TESTING = 500\n",
    "NUM_VALIDATION = 500\n",
    "\n",
    "NUM_FACE_TRAINING = 451\n",
    "NUM_FACE_VALIDATION = 301\n",
    "NUM_FACE_TESTING = 150\n",
    "\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data_file = \"data/digitdata/trainingimages\"\n",
    "train_label_file = \"data/digitdata/traininglabels\"\n",
    "val_data_file = \"data/digitdata/validationimages\"\n",
    "val_label_file = \"data/digitdata/validationlabels\"\n",
    "test_data_file = \"data/digitdata/testimages\"\n",
    "test_label_file = \"data/digitdata/testlabels\"\n",
    "\n",
    "face_train_data_file = \"data/facedata/facedatatrain\"\n",
    "face_train_label_file = \"data/facedata/facedatatrainlabels\"\n",
    "face_val_data_file   = \"data/facedata/facedatavalidation\"\n",
    "face_val_label_file  = \"data/facedata/facedatavalidationlabels\"\n",
    "face_test_data_file  = \"data/facedata/facedatatest\"\n",
    "face_test_label_file = \"data/facedata/facedatatestlabels\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53324bb2",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca12b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.rstrip(\"\\n\") for line in lines]\n",
    "\n",
    "def extract_features(raw_data):\n",
    "    features = []\n",
    "    for i in range(0, len(raw_data), 28):\n",
    "        image = raw_data[i:i+28]\n",
    "        feature = [1 if ch != ' ' else 0 for row in image for ch in row]\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [int(line.strip()) for line in lines]\n",
    "\n",
    "def load_dataset(data_file, label_file, size=None):\n",
    "    raw_data = read_data_file(data_file)\n",
    "    raw_labels = read_labels(label_file)\n",
    "\n",
    "    features = extract_features(raw_data)\n",
    "    if size is not None:\n",
    "        combined = list(zip(features, raw_labels))\n",
    "        random.shuffle(combined)\n",
    "        features, raw_labels = zip(*combined[:size])\n",
    "\n",
    "    return list(features), list(raw_labels)\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    encoded = np.zeros((num_classes, len(y)))\n",
    "    for idx, val in enumerate(y):\n",
    "        encoded[val][idx] = 1\n",
    "    return encoded\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    correct = sum(p == t for p, t in zip(predictions, labels))\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99491bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(raw_data):\n",
    "    features = []\n",
    "    for i in range(0, len(raw_data), 70):  # 70 rows per image\n",
    "        image = raw_data[i:i+70]\n",
    "        assert all(len(row) == 60 for row in image), \"Expected 60 columns per row in face image\"\n",
    "        feature = [1 if ch != ' ' else 0 for row in image for ch in row]\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def load_face_dataset(data_file, label_file, size=None):\n",
    "    raw_data = read_data_file(data_file)\n",
    "    raw_labels = read_labels(label_file)\n",
    "\n",
    "    features = extract_face_features(raw_data)\n",
    "    if size is not None:\n",
    "        combined = list(zip(features, raw_labels))\n",
    "        random.shuffle(combined)\n",
    "        features, raw_labels = zip(*combined[:size])\n",
    "\n",
    "    return list(features), list(raw_labels)\n",
    "\n",
    "def one_hot_encode_face(y, num_classes=2):\n",
    "    encoded = np.zeros((num_classes, len(y)))\n",
    "    for idx, val in enumerate(y):\n",
    "        encoded[val][idx] = 1\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03739294",
   "metadata": {},
   "source": [
    "# Three-Layer Neural Network: Manual Implementations of Forward Pass, Back-Propagation, and Weight Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3113e73",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d869892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return e_Z / np.sum(e_Z, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    return {\n",
    "        'W1': np.random.randn(hidden1_size, input_size) * 0.01,\n",
    "        'b1': np.zeros((hidden1_size, 1)),\n",
    "        'W2': np.random.randn(hidden2_size, hidden1_size) * 0.01,\n",
    "        'b2': np.zeros((hidden2_size, 1)),\n",
    "        'W3': np.random.randn(output_size, hidden2_size) * 0.01,\n",
    "        'b3': np.zeros((output_size, 1))\n",
    "    }\n",
    "\n",
    "# Forward pass\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    W3, b3 = parameters['W3'], parameters['b3']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    cache = (Z1, A1, Z2, A2, Z3, A3)\n",
    "    return A3, cache\n",
    "\n",
    "\n",
    "def forward_propagation_face(X, parameters, dropout_rate=0.5, training=True):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    W3, b3 = parameters['W3'], parameters['b3']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    if training:\n",
    "        D1 = (np.random.rand(*A1.shape) < dropout_rate).astype(float)\n",
    "        A1 *= D1\n",
    "        A1 /= dropout_rate\n",
    "    else:\n",
    "        D1 = None\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    if training:\n",
    "        D2 = (np.random.rand(*A2.shape) < dropout_rate).astype(float)\n",
    "        A2 *= D2\n",
    "        A2 /= dropout_rate\n",
    "    else:\n",
    "        D2 = None\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    # Include dropout masks in the cache\n",
    "    cache = (Z1, A1, D1, Z2, A2, D2, Z3, A3)\n",
    "    return A3, cache\n",
    "\n",
    "\n",
    "\n",
    "# Loss\n",
    "def compute_loss(Y_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum(Y * np.log(Y_hat + 1e-8) + (1 - Y) * np.log(1 - Y_hat + 1e-8)) / m\n",
    "\n",
    "def compute_loss_l2(Y_hat, Y, parameters, lambda_reg=0.1):\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
    "    l2 = (lambda_reg / (2 * m)) * (\n",
    "        np.sum(np.square(parameters['W1'])) +\n",
    "        np.sum(np.square(parameters['W2'])) +\n",
    "        np.sum(np.square(parameters['W3']))\n",
    "    )\n",
    "    return cross_entropy + l2\n",
    "\n",
    "# Backward pass\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]\n",
    "    W2, W3 = parameters['W2'], parameters['W3']\n",
    "    Z1, A1, Z2, A2, Z3, A3 = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW3': dW3, 'db3': db3\n",
    "    }\n",
    "\n",
    "\n",
    "def backward_propagation_face(X, Y, parameters, cache, dropout_rate=0.5):\n",
    "    m = X.shape[1]\n",
    "    W2, W3 = parameters['W2'], parameters['W3']\n",
    "    Z1, A1, D1, Z2, A2, D2, Z3, A3 = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dA2 *= D2\n",
    "    dA2 /= dropout_rate\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 *= D1\n",
    "    dA1 /= dropout_rate\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW3': dW3, 'db3': db3\n",
    "    }\n",
    "\n",
    "# Gradient descent update\n",
    "def update_parameters(params, grads, lr):\n",
    "    for key in params:\n",
    "        params[key] -= lr * grads['d' + key]\n",
    "    return params\n",
    "\n",
    "# Prediction\n",
    "def predict_nn(X, parameters):\n",
    "    Y_hat, _ = forward_propagation(X, parameters)\n",
    "    return np.argmax(Y_hat, axis=0)\n",
    "\n",
    "def predict_nn_face(X, parameters):\n",
    "    Y_hat, _ = forward_propagation_face(X, parameters, training=False)\n",
    "    return np.argmax(Y_hat, axis=0)\n",
    "\n",
    "# Training loop\n",
    "def train_neural_net(X_train, y_train, X_test, y_test,\n",
    "                     input_size, h1, h2, output_size,\n",
    "                     epochs=1000, lr=0.1, print_loss=True,\n",
    "                     X_val=None, y_val=None, early_stopping=False, patience=10):\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, h1, h2, output_size)\n",
    "    best_params = None\n",
    "    best_val_acc = 0\n",
    "    val_acc_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward and backpropagation\n",
    "        Y_hat, cache = forward_propagation(X_train, parameters)\n",
    "        loss = compute_loss(Y_hat, y_train)\n",
    "        grads = backward_propagation(X_train, y_train, parameters, cache)\n",
    "        parameters = update_parameters(parameters, grads, lr)\n",
    "\n",
    "        # Check performance every 100 epochs\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            train_preds = predict_nn(X_train, parameters)\n",
    "            train_acc = evaluate(train_preds, np.argmax(y_train, axis=0))\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_preds = predict_nn(X_val, parameters)\n",
    "                val_acc = evaluate(val_preds, np.argmax(y_val, axis=0))\n",
    "\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f} | Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "                # Save best model\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "                    val_acc_counter = 0\n",
    "                else:\n",
    "                    val_acc_counter += 1\n",
    "                    if early_stopping and val_acc_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "    final_params = best_params if best_params is not None else parameters\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_preds = predict_nn(X_test, final_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return final_params\n",
    "\n",
    "\n",
    "\n",
    "def train_neural_net_face(X_train, y_train, X_test, y_test,\n",
    "                     input_size, h1, h2, output_size,\n",
    "                     epochs=1000, lr=0.1, print_loss=True,\n",
    "                     X_val=None, y_val=None, early_stopping=False,\n",
    "                     patience=10, dropout_rate=0.5, lambda_reg=0.1):  \n",
    "    \n",
    "    parameters = initialize_parameters(input_size, h1, h2, output_size)\n",
    "    best_params = None\n",
    "    best_val_acc = 0\n",
    "    val_acc_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === DROPOUT + L2 ===\n",
    "        Y_hat, cache = forward_propagation_face(X_train, parameters, dropout_rate=dropout_rate, training=True)\n",
    "        loss = compute_loss_l2(Y_hat, y_train, parameters, lambda_reg=lambda_reg)\n",
    "        grads = backward_propagation_face(X_train, y_train, parameters, cache, dropout_rate=dropout_rate)\n",
    "        parameters = update_parameters(parameters, grads, lr)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            train_preds = predict_nn_face(X_train, parameters)\n",
    "            train_acc = evaluate(train_preds, np.argmax(y_train, axis=0))\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_preds = predict_nn_face(X_val, parameters)\n",
    "                val_acc = evaluate(val_preds, np.argmax(y_val, axis=0))\n",
    "\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f} | Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "                    val_acc_counter = 0\n",
    "                else:\n",
    "                    val_acc_counter += 1\n",
    "                    if early_stopping and val_acc_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "    final_params = best_params if best_params is not None else parameters\n",
    "    test_preds = predict_nn_face(X_test, final_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return final_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0affc",
   "metadata": {},
   "source": [
    "## Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87085246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural net on digit data\n",
      "\n",
      " DIGITS: Training on 100 samples (10%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1200 | Val Acc = 0.0900\n",
      "Epoch 100: Loss = 3.2256 | Train Acc = 0.1400 | Val Acc = 0.0960\n",
      "Epoch 200: Loss = 3.1577 | Train Acc = 0.2000 | Val Acc = 0.1460\n",
      "Epoch 300: Loss = 1.5617 | Train Acc = 0.7700 | Val Acc = 0.5240\n",
      "Epoch 400: Loss = 0.3130 | Train Acc = 1.0000 | Val Acc = 0.6300\n",
      "Epoch 500: Loss = 0.0584 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Epoch 600: Loss = 0.0249 | Train Acc = 1.0000 | Val Acc = 0.6400\n",
      "Epoch 700: Loss = 0.0146 | Train Acc = 1.0000 | Val Acc = 0.6420\n",
      "Epoch 800: Loss = 0.0099 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Epoch 900: Loss = 0.0074 | Train Acc = 1.0000 | Val Acc = 0.6400\n",
      "Epoch 999: Loss = 0.0058 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Final Test Accuracy: 0.5900\n",
      "DIGITS Test Accuracy with 100 samples: 0.5900\n",
      "\n",
      " DIGITS: Training on 200 samples (20%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1100 | Val Acc = 0.0840\n",
      "Epoch 100: Loss = 3.2451 | Train Acc = 0.2200 | Val Acc = 0.1360\n",
      "Epoch 200: Loss = 3.1422 | Train Acc = 0.1850 | Val Acc = 0.1620\n",
      "Epoch 300: Loss = 1.6565 | Train Acc = 0.7600 | Val Acc = 0.5780\n",
      "Epoch 400: Loss = 0.7981 | Train Acc = 0.9200 | Val Acc = 0.6480\n",
      "Epoch 500: Loss = 0.2367 | Train Acc = 0.9900 | Val Acc = 0.6920\n",
      "Epoch 600: Loss = 0.0718 | Train Acc = 1.0000 | Val Acc = 0.6960\n",
      "Epoch 700: Loss = 0.0334 | Train Acc = 1.0000 | Val Acc = 0.7000\n",
      "Epoch 800: Loss = 0.0201 | Train Acc = 1.0000 | Val Acc = 0.7020\n",
      "Epoch 900: Loss = 0.0139 | Train Acc = 1.0000 | Val Acc = 0.7040\n",
      "Epoch 999: Loss = 0.0104 | Train Acc = 1.0000 | Val Acc = 0.7000\n",
      "Final Test Accuracy: 0.6460\n",
      "DIGITS Test Accuracy with 200 samples: 0.6460\n",
      "\n",
      " DIGITS: Training on 300 samples (30%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0567 | Val Acc = 0.0500\n",
      "Epoch 100: Loss = 3.2504 | Train Acc = 0.1800 | Val Acc = 0.1240\n",
      "Epoch 200: Loss = 3.1732 | Train Acc = 0.2167 | Val Acc = 0.1400\n",
      "Epoch 300: Loss = 1.8543 | Train Acc = 0.7100 | Val Acc = 0.5620\n",
      "Epoch 400: Loss = 1.0484 | Train Acc = 0.8300 | Val Acc = 0.6700\n",
      "Epoch 500: Loss = 0.4200 | Train Acc = 0.9567 | Val Acc = 0.7240\n",
      "Epoch 600: Loss = 0.1445 | Train Acc = 1.0000 | Val Acc = 0.7240\n",
      "Epoch 700: Loss = 0.0607 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Epoch 800: Loss = 0.0335 | Train Acc = 1.0000 | Val Acc = 0.7260\n",
      "Epoch 900: Loss = 0.0218 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Epoch 999: Loss = 0.0157 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Final Test Accuracy: 0.6880\n",
      "DIGITS Test Accuracy with 300 samples: 0.6880\n",
      "\n",
      " DIGITS: Training on 400 samples (40%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0550 | Val Acc = 0.0540\n",
      "Epoch 100: Loss = 3.2637 | Train Acc = 0.1475 | Val Acc = 0.1160\n",
      "Epoch 200: Loss = 3.2236 | Train Acc = 0.1800 | Val Acc = 0.1540\n",
      "Epoch 300: Loss = 2.0163 | Train Acc = 0.6525 | Val Acc = 0.5820\n",
      "Epoch 400: Loss = 1.2303 | Train Acc = 0.8475 | Val Acc = 0.7100\n",
      "Epoch 500: Loss = 0.6272 | Train Acc = 0.9325 | Val Acc = 0.7640\n",
      "Epoch 600: Loss = 0.2507 | Train Acc = 0.9900 | Val Acc = 0.7440\n",
      "Epoch 700: Loss = 0.1061 | Train Acc = 1.0000 | Val Acc = 0.7420\n",
      "Epoch 800: Loss = 0.0537 | Train Acc = 1.0000 | Val Acc = 0.7480\n",
      "Epoch 900: Loss = 0.0326 | Train Acc = 1.0000 | Val Acc = 0.7560\n",
      "Epoch 999: Loss = 0.0223 | Train Acc = 1.0000 | Val Acc = 0.7600\n",
      "Final Test Accuracy: 0.6960\n",
      "DIGITS Test Accuracy with 400 samples: 0.6960\n",
      "\n",
      " DIGITS: Training on 500 samples (50%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0780 | Val Acc = 0.0960\n",
      "Epoch 100: Loss = 3.2599 | Train Acc = 0.1140 | Val Acc = 0.1100\n",
      "Epoch 200: Loss = 3.2236 | Train Acc = 0.1900 | Val Acc = 0.1580\n",
      "Epoch 300: Loss = 2.0686 | Train Acc = 0.6280 | Val Acc = 0.5540\n",
      "Epoch 400: Loss = 1.2890 | Train Acc = 0.8020 | Val Acc = 0.7020\n",
      "Epoch 500: Loss = 0.6860 | Train Acc = 0.9260 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.3405 | Train Acc = 0.9740 | Val Acc = 0.7900\n",
      "Epoch 700: Loss = 0.1629 | Train Acc = 0.9920 | Val Acc = 0.7860\n",
      "Epoch 800: Loss = 0.0816 | Train Acc = 1.0000 | Val Acc = 0.7880\n",
      "Epoch 900: Loss = 0.0468 | Train Acc = 1.0000 | Val Acc = 0.7840\n",
      "Epoch 999: Loss = 0.0308 | Train Acc = 1.0000 | Val Acc = 0.7880\n",
      "Final Test Accuracy: 0.7280\n",
      "DIGITS Test Accuracy with 500 samples: 0.7280\n",
      "\n",
      " DIGITS: Training on 600 samples (60%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0917 | Val Acc = 0.1000\n",
      "Epoch 100: Loss = 3.2607 | Train Acc = 0.1150 | Val Acc = 0.1100\n",
      "Epoch 200: Loss = 3.2213 | Train Acc = 0.2167 | Val Acc = 0.1900\n",
      "Epoch 300: Loss = 1.9933 | Train Acc = 0.6483 | Val Acc = 0.5640\n",
      "Epoch 400: Loss = 1.2723 | Train Acc = 0.8167 | Val Acc = 0.6980\n",
      "Epoch 500: Loss = 0.7867 | Train Acc = 0.8800 | Val Acc = 0.7420\n",
      "Epoch 600: Loss = 0.4155 | Train Acc = 0.9633 | Val Acc = 0.7580\n",
      "Epoch 700: Loss = 0.2156 | Train Acc = 0.9867 | Val Acc = 0.7660\n",
      "Epoch 800: Loss = 0.1129 | Train Acc = 0.9967 | Val Acc = 0.7660\n",
      "Epoch 900: Loss = 0.0635 | Train Acc = 1.0000 | Val Acc = 0.7700\n",
      "Epoch 999: Loss = 0.0402 | Train Acc = 1.0000 | Val Acc = 0.7720\n",
      "Final Test Accuracy: 0.7600\n",
      "DIGITS Test Accuracy with 600 samples: 0.7600\n",
      "\n",
      " DIGITS: Training on 700 samples (70%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1371 | Val Acc = 0.1480\n",
      "Epoch 100: Loss = 3.2629 | Train Acc = 0.1171 | Val Acc = 0.0780\n",
      "Epoch 200: Loss = 3.2096 | Train Acc = 0.1957 | Val Acc = 0.1900\n",
      "Epoch 300: Loss = 2.0864 | Train Acc = 0.5986 | Val Acc = 0.5500\n",
      "Epoch 400: Loss = 1.2862 | Train Acc = 0.8029 | Val Acc = 0.6920\n",
      "Epoch 500: Loss = 0.8117 | Train Acc = 0.8871 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.4909 | Train Acc = 0.9457 | Val Acc = 0.7800\n",
      "Epoch 700: Loss = 0.2760 | Train Acc = 0.9786 | Val Acc = 0.7740\n",
      "Epoch 800: Loss = 0.1538 | Train Acc = 0.9914 | Val Acc = 0.7820\n",
      "Epoch 900: Loss = 0.0869 | Train Acc = 1.0000 | Val Acc = 0.7960\n",
      "Epoch 999: Loss = 0.0539 | Train Acc = 1.0000 | Val Acc = 0.8000\n",
      "Final Test Accuracy: 0.7560\n",
      "DIGITS Test Accuracy with 700 samples: 0.7560\n",
      "\n",
      " DIGITS: Training on 800 samples (80%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1363 | Val Acc = 0.1520\n",
      "Epoch 100: Loss = 3.2629 | Train Acc = 0.1163 | Val Acc = 0.0780\n",
      "Epoch 200: Loss = 3.2119 | Train Acc = 0.1913 | Val Acc = 0.1840\n",
      "Epoch 300: Loss = 2.1137 | Train Acc = 0.5962 | Val Acc = 0.5520\n",
      "Epoch 400: Loss = 1.2931 | Train Acc = 0.8013 | Val Acc = 0.7040\n",
      "Epoch 500: Loss = 0.8307 | Train Acc = 0.8838 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.5243 | Train Acc = 0.9375 | Val Acc = 0.7920\n",
      "Epoch 700: Loss = 0.3128 | Train Acc = 0.9738 | Val Acc = 0.8000\n",
      "Epoch 800: Loss = 0.1811 | Train Acc = 0.9888 | Val Acc = 0.8080\n",
      "Epoch 900: Loss = 0.1049 | Train Acc = 0.9975 | Val Acc = 0.8100\n",
      "Epoch 999: Loss = 0.0641 | Train Acc = 1.0000 | Val Acc = 0.8180\n",
      "Final Test Accuracy: 0.7640\n",
      "DIGITS Test Accuracy with 800 samples: 0.7640\n",
      "\n",
      " DIGITS: Training on 900 samples (90%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1178 | Val Acc = 0.1380\n",
      "Epoch 100: Loss = 3.2622 | Train Acc = 0.1122 | Val Acc = 0.0800\n",
      "Epoch 200: Loss = 3.2113 | Train Acc = 0.2300 | Val Acc = 0.2060\n",
      "Epoch 300: Loss = 2.0194 | Train Acc = 0.5889 | Val Acc = 0.5340\n",
      "Epoch 400: Loss = 1.2823 | Train Acc = 0.7956 | Val Acc = 0.7060\n",
      "Epoch 500: Loss = 0.8499 | Train Acc = 0.8767 | Val Acc = 0.7640\n",
      "Epoch 600: Loss = 0.5574 | Train Acc = 0.9333 | Val Acc = 0.8000\n",
      "Epoch 700: Loss = 0.3543 | Train Acc = 0.9656 | Val Acc = 0.8080\n",
      "Epoch 800: Loss = 0.2110 | Train Acc = 0.9867 | Val Acc = 0.8180\n",
      "Epoch 900: Loss = 0.1278 | Train Acc = 0.9944 | Val Acc = 0.8220\n",
      "Epoch 999: Loss = 0.0794 | Train Acc = 1.0000 | Val Acc = 0.8280\n",
      "Final Test Accuracy: 0.7740\n",
      "DIGITS Test Accuracy with 900 samples: 0.7740\n",
      "\n",
      " DIGITS: Training on 1000 samples (100%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1150 | Val Acc = 0.1300\n",
      "Epoch 100: Loss = 3.2654 | Train Acc = 0.1120 | Val Acc = 0.1000\n",
      "Epoch 200: Loss = 3.2206 | Train Acc = 0.2240 | Val Acc = 0.2000\n",
      "Epoch 300: Loss = 2.0562 | Train Acc = 0.5750 | Val Acc = 0.5260\n",
      "Epoch 400: Loss = 1.3209 | Train Acc = 0.7800 | Val Acc = 0.6980\n",
      "Epoch 500: Loss = 0.8731 | Train Acc = 0.8640 | Val Acc = 0.7820\n",
      "Epoch 600: Loss = 0.5899 | Train Acc = 0.9220 | Val Acc = 0.8140\n",
      "Epoch 700: Loss = 0.3715 | Train Acc = 0.9590 | Val Acc = 0.8200\n",
      "Epoch 800: Loss = 0.2315 | Train Acc = 0.9820 | Val Acc = 0.8360\n",
      "Epoch 900: Loss = 0.1428 | Train Acc = 0.9940 | Val Acc = 0.8400\n",
      "Epoch 999: Loss = 0.0914 | Train Acc = 0.9970 | Val Acc = 0.8400\n",
      "Final Test Accuracy: 0.7940\n",
      "DIGITS Test Accuracy with 1000 samples: 0.7940\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing neural net on digit data\")\n",
    "\n",
    "X_train_raw, y_train_raw = load_dataset(train_data_file, train_label_file, size=NUM_TRAINING)\n",
    "X_val_raw, y_val_raw = load_dataset(val_data_file, val_label_file, size=NUM_VALIDATION)\n",
    "X_test_raw, y_test_raw = load_dataset(test_data_file, test_label_file, size=NUM_TESTING)\n",
    "\n",
    "X_train = np.array(X_train_raw).T\n",
    "X_val = np.array(X_val_raw).T\n",
    "X_test = np.array(X_test_raw).T\n",
    "\n",
    "y_train = one_hot_encode(y_train_raw)\n",
    "y_val = one_hot_encode(y_val_raw)\n",
    "y_test = one_hot_encode(y_test_raw)\n",
    "\n",
    "# Train on increasing percentages of DIGIT data \n",
    "percentages = [0.1 * i for i in range(1, 11)]  # 10% to 100%\n",
    "total_digit_samples = X_train.shape[1]\n",
    "\n",
    "digit_results = []\n",
    "\n",
    "for pct in percentages:\n",
    "    n = int(pct * total_digit_samples)    \n",
    "    X_subset = X_train[:, :n]\n",
    "    y_subset = y_train[:, :n]\n",
    "\n",
    "    print(f\"\\n DIGITS: Training on {n} samples ({int(pct * 100)}%)\")\n",
    "    \n",
    "    trained_params = train_neural_net(\n",
    "        X_subset, y_subset,\n",
    "        X_test, y_test,\n",
    "        input_size=784, h1=128, h2=64, output_size=10,\n",
    "        epochs=1000, lr=0.1,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        early_stopping=True, patience=10\n",
    "    )\n",
    "\n",
    "    test_preds = predict_nn(X_test, trained_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    digit_results.append((n, test_acc))\n",
    "    print(f\"DIGITS Test Accuracy with {n} samples: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef98d09",
   "metadata": {},
   "source": [
    "## Face Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5dfbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural net on face data\n",
      "\n",
      " FACES: Training on 45 samples (10%)\n",
      "Epoch 0: Loss = 0.7682 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7618 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7194 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.1878 | Train Acc = 1.0000 | Val Acc = 0.7143\n",
      "Epoch 400: Loss = 0.1487 | Train Acc = 1.0000 | Val Acc = 0.6910\n",
      "Epoch 500: Loss = 0.1312 | Train Acc = 1.0000 | Val Acc = 0.6146\n",
      "Epoch 600: Loss = 0.1297 | Train Acc = 1.0000 | Val Acc = 0.6478\n",
      "Epoch 700: Loss = 0.1343 | Train Acc = 1.0000 | Val Acc = 0.6744\n",
      "Epoch 800: Loss = 0.1378 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Epoch 900: Loss = 0.1354 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Epoch 999: Loss = 0.1407 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Final Test Accuracy: 0.6733\n",
      "FACES Test Accuracy with 45 samples: 0.6733\n",
      "\n",
      " FACES: Training on 90 samples (20%)\n",
      "Epoch 0: Loss = 0.7307 | Train Acc = 0.5000 | Val Acc = 0.4751\n",
      "Epoch 100: Loss = 0.7306 | Train Acc = 0.7444 | Val Acc = 0.5415\n",
      "Epoch 200: Loss = 0.7269 | Train Acc = 0.9556 | Val Acc = 0.6113\n",
      "Epoch 300: Loss = 0.1093 | Train Acc = 1.0000 | Val Acc = 0.7409\n",
      "Epoch 400: Loss = 0.0938 | Train Acc = 1.0000 | Val Acc = 0.7409\n",
      "Epoch 500: Loss = 0.0881 | Train Acc = 1.0000 | Val Acc = 0.7243\n",
      "Epoch 600: Loss = 0.0842 | Train Acc = 1.0000 | Val Acc = 0.7243\n",
      "Epoch 700: Loss = 0.0766 | Train Acc = 1.0000 | Val Acc = 0.6910\n",
      "Epoch 800: Loss = 0.0827 | Train Acc = 1.0000 | Val Acc = 0.6844\n",
      "Epoch 900: Loss = 0.1012 | Train Acc = 1.0000 | Val Acc = 0.6944\n",
      "Epoch 999: Loss = 0.0807 | Train Acc = 1.0000 | Val Acc = 0.6611\n",
      "Final Test Accuracy: 0.6933\n",
      "FACES Test Accuracy with 90 samples: 0.6933\n",
      "\n",
      " FACES: Training on 135 samples (30%)\n",
      "Epoch 0: Loss = 0.7182 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7168 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7164 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6078 | Train Acc = 0.9481 | Val Acc = 0.5714\n",
      "Epoch 400: Loss = 0.1291 | Train Acc = 1.0000 | Val Acc = 0.7907\n",
      "Epoch 500: Loss = 0.0879 | Train Acc = 1.0000 | Val Acc = 0.7708\n",
      "Epoch 600: Loss = 0.0609 | Train Acc = 1.0000 | Val Acc = 0.7807\n",
      "Epoch 700: Loss = 0.0706 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Epoch 800: Loss = 0.0621 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Epoch 900: Loss = 0.0748 | Train Acc = 1.0000 | Val Acc = 0.7641\n",
      "Epoch 999: Loss = 0.0590 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Final Test Accuracy: 0.7467\n",
      "FACES Test Accuracy with 135 samples: 0.7467\n",
      "\n",
      " FACES: Training on 180 samples (40%)\n",
      "Epoch 0: Loss = 0.7119 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7112 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7109 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.5626 | Train Acc = 0.9778 | Val Acc = 0.7110\n",
      "Epoch 400: Loss = 0.1048 | Train Acc = 1.0000 | Val Acc = 0.8272\n",
      "Epoch 500: Loss = 0.0661 | Train Acc = 1.0000 | Val Acc = 0.8073\n",
      "Epoch 600: Loss = 0.0570 | Train Acc = 1.0000 | Val Acc = 0.8106\n",
      "Epoch 700: Loss = 0.0480 | Train Acc = 1.0000 | Val Acc = 0.8007\n",
      "Epoch 800: Loss = 0.0408 | Train Acc = 1.0000 | Val Acc = 0.7973\n",
      "Epoch 900: Loss = 0.0464 | Train Acc = 1.0000 | Val Acc = 0.7907\n",
      "Epoch 999: Loss = 0.0453 | Train Acc = 1.0000 | Val Acc = 0.7741\n",
      "Final Test Accuracy: 0.8067\n",
      "FACES Test Accuracy with 180 samples: 0.8067\n",
      "\n",
      " FACES: Training on 225 samples (50%)\n",
      "Epoch 0: Loss = 0.7082 | Train Acc = 0.5156 | Val Acc = 0.5249\n",
      "Epoch 100: Loss = 0.7081 | Train Acc = 0.5022 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7079 | Train Acc = 0.5022 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6985 | Train Acc = 0.8978 | Val Acc = 0.6811\n",
      "Epoch 400: Loss = 0.1540 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 500: Loss = 0.0799 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 600: Loss = 0.0544 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 700: Loss = 0.0367 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 800: Loss = 0.0459 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 900: Loss = 0.0441 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 999: Loss = 0.0413 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Final Test Accuracy: 0.8800\n",
      "FACES Test Accuracy with 225 samples: 0.8800\n",
      "\n",
      " FACES: Training on 270 samples (60%)\n",
      "Epoch 0: Loss = 0.7056 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7039 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7037 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7012 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.2080 | Train Acc = 1.0000 | Val Acc = 0.8571\n",
      "Epoch 500: Loss = 0.0865 | Train Acc = 1.0000 | Val Acc = 0.8671\n",
      "Epoch 600: Loss = 0.0503 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 700: Loss = 0.0451 | Train Acc = 1.0000 | Val Acc = 0.8605\n",
      "Epoch 800: Loss = 0.0401 | Train Acc = 1.0000 | Val Acc = 0.8439\n",
      "Epoch 900: Loss = 0.0376 | Train Acc = 1.0000 | Val Acc = 0.8505\n",
      "Epoch 999: Loss = 0.0388 | Train Acc = 1.0000 | Val Acc = 0.8472\n",
      "Final Test Accuracy: 0.8733\n",
      "FACES Test Accuracy with 270 samples: 0.8733\n",
      "\n",
      " FACES: Training on 315 samples (70%)\n",
      "Epoch 0: Loss = 0.7038 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7032 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7031 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7008 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.2038 | Train Acc = 1.0000 | Val Acc = 0.8771\n",
      "Epoch 500: Loss = 0.0844 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 600: Loss = 0.0550 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 700: Loss = 0.0422 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 800: Loss = 0.0383 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 900: Loss = 0.0428 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 999: Loss = 0.0364 | Train Acc = 1.0000 | Val Acc = 0.8472\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 315 samples: 0.8600\n",
      "\n",
      " FACES: Training on 360 samples (80%)\n",
      "Epoch 0: Loss = 0.7025 | Train Acc = 0.4972 | Val Acc = 0.5150\n",
      "Epoch 100: Loss = 0.7025 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7023 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7011 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.4001 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 500: Loss = 0.1075 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 600: Loss = 0.0500 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 700: Loss = 0.0462 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 800: Loss = 0.0536 | Train Acc = 1.0000 | Val Acc = 0.8738\n",
      "Epoch 900: Loss = 0.0498 | Train Acc = 1.0000 | Val Acc = 0.8571\n",
      "Epoch 999: Loss = 0.0364 | Train Acc = 1.0000 | Val Acc = 0.8605\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 360 samples: 0.8600\n",
      "\n",
      " FACES: Training on 405 samples (90%)\n",
      "Epoch 0: Loss = 0.7015 | Train Acc = 0.5012 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7014 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7012 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6999 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.3557 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 500: Loss = 0.0966 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 600: Loss = 0.0605 | Train Acc = 1.0000 | Val Acc = 0.8937\n",
      "Epoch 700: Loss = 0.0347 | Train Acc = 1.0000 | Val Acc = 0.8937\n",
      "Epoch 800: Loss = 0.0318 | Train Acc = 1.0000 | Val Acc = 0.8904\n",
      "Epoch 900: Loss = 0.0311 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 999: Loss = 0.0267 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Final Test Accuracy: 0.8667\n",
      "FACES Test Accuracy with 405 samples: 0.8667\n",
      "\n",
      " FACES: Training on 451 samples (100%)\n",
      "Epoch 0: Loss = 0.7006 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.6999 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.6999 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6991 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.6038 | Train Acc = 0.9534 | Val Acc = 0.8073\n",
      "Epoch 500: Loss = 0.1134 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 600: Loss = 0.0777 | Train Acc = 1.0000 | Val Acc = 0.8738\n",
      "Epoch 700: Loss = 0.0492 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 800: Loss = 0.0495 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 900: Loss = 0.0279 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 999: Loss = 0.0299 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 451 samples: 0.8600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Testing neural net on face data\")\n",
    "\n",
    "# Load and process face data\n",
    "X_face_train_raw, y_face_train_raw = load_face_dataset(face_train_data_file, face_train_label_file, size=NUM_FACE_TRAINING)\n",
    "X_face_val_raw, y_face_val_raw = load_face_dataset(face_val_data_file, face_val_label_file, size=NUM_FACE_VALIDATION)\n",
    "X_face_test_raw, y_face_test_raw = load_face_dataset(face_test_data_file, face_test_label_file, size=NUM_FACE_TESTING)\n",
    "\n",
    "X_face_train = np.array(X_face_train_raw).T\n",
    "X_face_val = np.array(X_face_val_raw).T\n",
    "X_face_test = np.array(X_face_test_raw).T\n",
    "\n",
    "y_face_train = one_hot_encode_face(y_face_train_raw)\n",
    "y_face_val = one_hot_encode_face(y_face_val_raw)\n",
    "y_face_test = one_hot_encode_face(y_face_test_raw)\n",
    "\n",
    "# Train on increasing percentages of FACE data \n",
    "percentages = [0.1 * i for i in range(1, 11)]  # 10% to 100%\n",
    "total_face_samples = X_face_train.shape[1]\n",
    "\n",
    "face_results = []\n",
    "\n",
    "for pct in percentages:\n",
    "    n = int(pct * total_face_samples)\n",
    "\n",
    "    X_subset = X_face_train[:, :n]\n",
    "    y_subset = y_face_train[:, :n]\n",
    "\n",
    "    print(f\"\\n FACES: Training on {n} samples ({int(pct * 100)}%)\")\n",
    "\n",
    "    trained_params = train_neural_net_face(\n",
    "        X_subset, y_subset,\n",
    "        X_face_test, y_face_test,\n",
    "        input_size=4200, h1=32, h2=16, output_size=2,\n",
    "        epochs=1000, lr=0.1,\n",
    "        X_val=X_face_val, y_val=y_face_val,\n",
    "        early_stopping=True, patience=10,\n",
    "        dropout_rate=0.5,  # dropout to decrease overfitting   \n",
    "        lambda_reg=0.5  #l2 regularization to decrease overfitting\n",
    "    )\n",
    "\n",
    "\n",
    "    test_preds = predict_nn_face(X_face_test, trained_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_face_test, axis=0))\n",
    "    face_results.append((n, test_acc))\n",
    "    print(f\"FACES Test Accuracy with {n} samples: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5fd6c",
   "metadata": {},
   "source": [
    "# Three-Layer Neural Network: PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e43b05",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6ba80ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, dtype=torch.float32):\n",
    "    return torch.tensor(np.asarray(x), dtype=dtype)\n",
    "\n",
    "def make_loader(X, y, batch_size=32, shuffle=True):\n",
    "    ds = TensorDataset(to_tensor(X), torch.tensor(y, dtype=torch.long))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ab0cd",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92efa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three-layer neural network w/ dropout\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, h1, h2, n_out, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim_in, h1),\n",
    "            nn.BatchNorm1d(h1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(h2, n_out)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "32dba34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNetwork(nn.Module):\n",
    "    def __init__(self, in_shape, n_out, dropout=0.5): # in_shape formatted like (channels, height, width)\n",
    "        super().__init__()\n",
    "        C, H, W = in_shape\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Unflatten(dim=1, unflattened_size=in_shape),\n",
    "            nn.Conv2d(C, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        H2, W2 = H // 2, W // 2 # after pool, divide by 2\n",
    "        fc_in  = 32 * H2 * W2\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(fc_in, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, n_out) # output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3d3f62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, loader, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# training loop\n",
    "def train(model, train_loader, val_loader, epochs=200, lr=1e-3, patience=15, weight_decay=1e-4, log_every=10, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc, best_state, bad_epochs = 0.0, None, 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running_loss += loss.item() * yb.size(0)\n",
    "\n",
    "        train_acc = get_accuracy(model, train_loader, device)\n",
    "        val_acc = get_accuracy(model, val_loader, device)\n",
    "\n",
    "        if epoch % log_every == 0 or epoch == epochs:\n",
    "            avg_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"Epoch {epoch:3d}: \"\n",
    "                  f\"Loss = {avg_loss:.4f} | \"\n",
    "                  f\"Train Acc = {train_acc:.4f} | \"\n",
    "                  f\"Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc + 1e-5:\n",
    "            best_val_acc, best_state = val_acc, model.state_dict()\n",
    "            bad_epochs = 0\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if bad_epochs >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, best_val_acc\n",
    "\n",
    "def test_neural_net(X_train, y_train, X_val, y_val, X_test, y_test, channels, height, width, n_out,\n",
    "                    dropout, lr, epochs, weight_decay, patience):\n",
    "    results = []\n",
    "    fractions = np.linspace(0.1, 1.0, 10)\n",
    "    for frac in fractions:\n",
    "        n = int(frac * len(y_train))\n",
    "        print(f\"\\nTraining on {n} samples ({frac:.0%})\")\n",
    "        train_loader = make_loader(X_train[:n], y_train[:n])\n",
    "        val_loader = make_loader(X_val, y_val, shuffle=True)\n",
    "        test_loader = make_loader(X_test, y_test, shuffle=True)\n",
    "\n",
    "        # model = MLP(dim_in, h1, h2, n_out, dropout)\n",
    "        model = ClassificationNetwork((channels, height, width), n_out)\n",
    "        model, _ = train(model, train_loader, val_loader, lr=lr, epochs=epochs, weight_decay=weight_decay, patience=patience)\n",
    "\n",
    "        # final test\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_acc = np.mean([\n",
    "                accuracy(model(xb), yb) for xb, yb in test_loader\n",
    "            ])\n",
    "\n",
    "        print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "        results.append((n, test_acc))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5840b8a",
   "metadata": {},
   "source": [
    "## Digit Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "582b425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 100 samples (10%)\n",
      "Epoch  10: Loss = 0.0349 | Train Acc = 1.0000 | Val Acc = 0.6840\n",
      "Epoch  20: Loss = 0.0031 | Train Acc = 1.0000 | Val Acc = 0.7300\n",
      "Epoch  30: Loss = 0.0013 | Train Acc = 1.0000 | Val Acc = 0.7320\n",
      "Epoch  40: Loss = 0.0011 | Train Acc = 1.0000 | Val Acc = 0.7220\n",
      "Epoch  50: Loss = 0.0013 | Train Acc = 1.0000 | Val Acc = 0.7220\n",
      "Epoch  60: Loss = 0.0011 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Epoch  70: Loss = 0.2841 | Train Acc = 0.6300 | Val Acc = 0.4520\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.6770\n",
      "\n",
      "Training on 200 samples (20%)\n",
      "Epoch  10: Loss = 0.0300 | Train Acc = 1.0000 | Val Acc = 0.8020\n",
      "Epoch  20: Loss = 0.0057 | Train Acc = 1.0000 | Val Acc = 0.7860\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.7324\n",
      "\n",
      "Training on 300 samples (30%)\n",
      "Epoch  10: Loss = 0.0265 | Train Acc = 1.0000 | Val Acc = 0.8440\n",
      "Epoch  20: Loss = 0.0148 | Train Acc = 1.0000 | Val Acc = 0.8420\n",
      "Epoch  30: Loss = 0.0034 | Train Acc = 1.0000 | Val Acc = 0.8580\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8020\n",
      "\n",
      "Training on 400 samples (40%)\n",
      "Epoch  10: Loss = 0.0364 | Train Acc = 1.0000 | Val Acc = 0.8760\n",
      "Epoch  20: Loss = 0.0059 | Train Acc = 1.0000 | Val Acc = 0.8920\n",
      "Epoch  30: Loss = 0.0012 | Train Acc = 1.0000 | Val Acc = 0.8820\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8297\n",
      "\n",
      "Training on 500 samples (50%)\n",
      "Epoch  10: Loss = 0.0892 | Train Acc = 0.9920 | Val Acc = 0.8520\n",
      "Epoch  20: Loss = 0.0078 | Train Acc = 1.0000 | Val Acc = 0.9100\n",
      "Epoch  30: Loss = 0.0181 | Train Acc = 1.0000 | Val Acc = 0.8960\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8137\n",
      "\n",
      "Training on 600 samples (60%)\n",
      "Epoch  10: Loss = 0.0239 | Train Acc = 0.9983 | Val Acc = 0.9200\n",
      "Epoch  20: Loss = 0.0248 | Train Acc = 1.0000 | Val Acc = 0.9200\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8613\n",
      "\n",
      "Training on 700 samples (70%)\n",
      "Epoch  10: Loss = 0.0269 | Train Acc = 1.0000 | Val Acc = 0.9260\n",
      "Epoch  20: Loss = 0.0233 | Train Acc = 0.9971 | Val Acc = 0.9120\n",
      "Epoch  30: Loss = 0.0222 | Train Acc = 1.0000 | Val Acc = 0.9240\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8887\n",
      "\n",
      "Training on 800 samples (80%)\n",
      "Epoch  10: Loss = 0.0374 | Train Acc = 0.9988 | Val Acc = 0.9160\n",
      "Epoch  20: Loss = 0.0316 | Train Acc = 0.9975 | Val Acc = 0.9100\n",
      "Epoch  30: Loss = 0.0210 | Train Acc = 0.9975 | Val Acc = 0.8880\n",
      "Epoch  40: Loss = 0.0095 | Train Acc = 1.0000 | Val Acc = 0.9300\n",
      "Epoch  50: Loss = 0.0152 | Train Acc = 1.0000 | Val Acc = 0.9380\n",
      "Epoch  60: Loss = 0.0058 | Train Acc = 1.0000 | Val Acc = 0.9380\n",
      "Epoch  70: Loss = 0.0127 | Train Acc = 1.0000 | Val Acc = 0.9300\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8828\n",
      "\n",
      "Training on 900 samples (90%)\n",
      "Epoch  10: Loss = 0.0371 | Train Acc = 1.0000 | Val Acc = 0.9100\n",
      "Epoch  20: Loss = 0.0178 | Train Acc = 0.9989 | Val Acc = 0.9180\n",
      "Epoch  30: Loss = 0.0283 | Train Acc = 0.9922 | Val Acc = 0.9120\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9164\n",
      "\n",
      "Training on 1000 samples (100%)\n",
      "Epoch  10: Loss = 0.0430 | Train Acc = 0.9930 | Val Acc = 0.9220\n",
      "Epoch  20: Loss = 0.0202 | Train Acc = 0.9860 | Val Acc = 0.9040\n",
      "Epoch  30: Loss = 0.0144 | Train Acc = 1.0000 | Val Acc = 0.9280\n",
      "Epoch  40: Loss = 0.0243 | Train Acc = 0.9980 | Val Acc = 0.9220\n",
      "Epoch  50: Loss = 0.0058 | Train Acc = 1.0000 | Val Acc = 0.9300\n",
      "Epoch  60: Loss = 0.0111 | Train Acc = 1.0000 | Val Acc = 0.9260\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9043\n"
     ]
    }
   ],
   "source": [
    "X_digit_train_raw, y_digit_train_raw = load_dataset(train_data_file, train_label_file, size=NUM_TRAINING)\n",
    "X_digit_val_raw, y_digit_val_raw = load_dataset(val_data_file, val_label_file, size=NUM_VALIDATION)\n",
    "X_digit_test_raw, y_digit_test_raw = load_dataset(test_data_file, test_label_file, size=NUM_TESTING)\n",
    "\n",
    "Xd_train, yd_train = np.array(X_digit_train_raw), np.array(y_digit_train_raw)\n",
    "Xd_val, yd_val = np.array(X_digit_val_raw), np.array(y_digit_val_raw)\n",
    "Xd_test, yd_test = np.array(X_digit_test_raw), np.array(y_digit_test_raw)\n",
    "\n",
    "digit_results = test_neural_net(\n",
    "    Xd_train, yd_train, Xd_val, yd_val, Xd_test, yd_test,\n",
    "    channels=1, height=28, width=28, n_out=10,\n",
    "    dropout=0.1,\n",
    "    lr=0.005, epochs=100,\n",
    "    weight_decay=1e-4, patience=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bae38",
   "metadata": {},
   "source": [
    "## Face Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a4214f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on 45 samples (10%)\n",
      "Epoch  10: Loss = 0.3688 | Train Acc = 1.0000 | Val Acc = 0.5216\n",
      "Epoch  20: Loss = 0.0000 | Train Acc = 1.0000 | Val Acc = 0.8904\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9125\n",
      "\n",
      "Training on 90 samples (20%)\n",
      "Epoch  10: Loss = 0.6484 | Train Acc = 0.4889 | Val Acc = 0.5183\n",
      "Epoch  20: Loss = 0.0010 | Train Acc = 1.0000 | Val Acc = 0.9402\n",
      "Epoch  30: Loss = 0.0058 | Train Acc = 1.0000 | Val Acc = 0.8970\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.8443\n",
      "\n",
      "Training on 135 samples (30%)\n",
      "Epoch  10: Loss = 0.0021 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch  20: Loss = 0.0029 | Train Acc = 1.0000 | Val Acc = 0.9402\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9472\n",
      "\n",
      "Training on 180 samples (40%)\n",
      "Epoch  10: Loss = 0.0039 | Train Acc = 1.0000 | Val Acc = 0.9302\n",
      "Epoch  20: Loss = 0.0051 | Train Acc = 1.0000 | Val Acc = 0.9468\n",
      "Epoch  30: Loss = 0.0138 | Train Acc = 1.0000 | Val Acc = 0.9402\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.6750\n",
      "\n",
      "Training on 225 samples (50%)\n",
      "Epoch  10: Loss = 0.0061 | Train Acc = 1.0000 | Val Acc = 0.9668\n",
      "Epoch  20: Loss = 0.0042 | Train Acc = 1.0000 | Val Acc = 0.9601\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9625\n",
      "\n",
      "Training on 270 samples (60%)\n",
      "Epoch  10: Loss = 0.0052 | Train Acc = 1.0000 | Val Acc = 0.9635\n",
      "Epoch  20: Loss = 0.0231 | Train Acc = 1.0000 | Val Acc = 0.9635\n",
      "Epoch  30: Loss = 0.0019 | Train Acc = 1.0000 | Val Acc = 0.9502\n",
      "Epoch  40: Loss = 0.0047 | Train Acc = 1.0000 | Val Acc = 0.9568\n",
      "Epoch  50: Loss = 0.0076 | Train Acc = 1.0000 | Val Acc = 0.9568\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9472\n",
      "\n",
      "Training on 315 samples (70%)\n",
      "Epoch  10: Loss = 0.0074 | Train Acc = 1.0000 | Val Acc = 0.9734\n",
      "Epoch  20: Loss = 0.0236 | Train Acc = 1.0000 | Val Acc = 0.9734\n",
      "Epoch  30: Loss = 0.0080 | Train Acc = 1.0000 | Val Acc = 0.9302\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9477\n",
      "\n",
      "Training on 360 samples (80%)\n",
      "Epoch  10: Loss = 0.0162 | Train Acc = 1.0000 | Val Acc = 0.9635\n",
      "Epoch  20: Loss = 0.0047 | Train Acc = 1.0000 | Val Acc = 0.9701\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9688\n",
      "\n",
      "Training on 405 samples (90%)\n",
      "Epoch  10: Loss = 0.0113 | Train Acc = 1.0000 | Val Acc = 0.9701\n",
      "Epoch  20: Loss = 0.0078 | Train Acc = 1.0000 | Val Acc = 0.9668\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9688\n",
      "\n",
      "Training on 451 samples (100%)\n",
      "Epoch  10: Loss = 0.0098 | Train Acc = 1.0000 | Val Acc = 0.9668\n",
      "Epoch  20: Loss = 0.0037 | Train Acc = 1.0000 | Val Acc = 0.9668\n",
      "Epoch  30: Loss = 0.0029 | Train Acc = 1.0000 | Val Acc = 0.9734\n",
      "Early stopping triggered.\n",
      "Final Test Accuracy: 0.9563\n"
     ]
    }
   ],
   "source": [
    "X_face_train_raw, y_face_train_raw = load_face_dataset(face_train_data_file, face_train_label_file, size=NUM_FACE_TRAINING)\n",
    "X_face_val_raw, y_face_val_raw = load_face_dataset(face_val_data_file, face_val_label_file, size=NUM_FACE_VALIDATION)\n",
    "X_face_test_raw, y_face_test_raw = load_face_dataset(face_test_data_file, face_test_label_file, size=NUM_FACE_TESTING)\n",
    "\n",
    "Xd_train, yd_train = np.array(X_face_train_raw), np.array(y_face_train_raw)\n",
    "Xd_val, yd_val = np.array(X_face_val_raw), np.array(y_face_val_raw)\n",
    "Xd_test, yd_test = np.array(X_face_test_raw), np.array(y_face_test_raw)\n",
    "\n",
    "digit_results = test_neural_net(\n",
    "    Xd_train, yd_train, Xd_val, yd_val, Xd_test, yd_test,\n",
    "    channels=1, height=70, width=60, n_out=2,\n",
    "    dropout=0.3,\n",
    "    lr=0.01, epochs=100,\n",
    "    weight_decay=1e-3, patience=15\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
