{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ce080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    with zipfile.ZipFile(\"data.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "NUM_TRAINING = 1000\n",
    "NUM_TESTING = 500\n",
    "NUM_VALIDATION = 500\n",
    "\n",
    "NUM_FACE_TRAINING = 451\n",
    "NUM_FACE_VALIDATION = 301\n",
    "NUM_FACE_TESTING = 150\n",
    "\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_WIDTH = 28\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepaths\n",
    "train_data_file = \"data/digitdata/trainingimages\"\n",
    "train_label_file = \"data/digitdata/traininglabels\"\n",
    "val_data_file = \"data/digitdata/validationimages\"\n",
    "val_label_file = \"data/digitdata/validationlabels\"\n",
    "test_data_file = \"data/digitdata/testimages\"\n",
    "test_label_file = \"data/digitdata/testlabels\"\n",
    "\n",
    "face_train_data_file = \"data/facedata/facedatatrain\"\n",
    "face_train_label_file = \"data/facedata/facedatatrainlabels\"\n",
    "face_val_data_file   = \"data/facedata/facedatavalidation\"\n",
    "face_val_label_file  = \"data/facedata/facedatavalidationlabels\"\n",
    "face_test_data_file  = \"data/facedata/facedatatest\"\n",
    "face_test_label_file = \"data/facedata/facedatatestlabels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1f45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53324bb2",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca12b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_data_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [line.rstrip(\"\\n\") for line in lines]\n",
    "\n",
    "def extract_features(raw_data):\n",
    "    features = []\n",
    "    for i in range(0, len(raw_data), 28):\n",
    "        image = raw_data[i:i+28]\n",
    "        feature = [1 if ch != ' ' else 0 for row in image for ch in row]\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def read_labels(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [int(line.strip()) for line in lines]\n",
    "\n",
    "def load_dataset(data_file, label_file, size=None):\n",
    "    raw_data = read_data_file(data_file)\n",
    "    raw_labels = read_labels(label_file)\n",
    "\n",
    "    features = extract_features(raw_data)\n",
    "    if size is not None:\n",
    "        combined = list(zip(features, raw_labels))\n",
    "        random.shuffle(combined)\n",
    "        features, raw_labels = zip(*combined[:size])\n",
    "\n",
    "    return list(features), list(raw_labels)\n",
    "\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    encoded = np.zeros((num_classes, len(y)))\n",
    "    for idx, val in enumerate(y):\n",
    "        encoded[val][idx] = 1\n",
    "    return encoded\n",
    "\n",
    "def evaluate(predictions, labels):\n",
    "    correct = sum(p == t for p, t in zip(predictions, labels))\n",
    "    return correct / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99491bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(raw_data):\n",
    "    features = []\n",
    "    for i in range(0, len(raw_data), 70):  # 60 rows per image\n",
    "        image = raw_data[i:i+70]\n",
    "        assert all(len(row) == 60 for row in image), \"Expected 60 columns per row in face image\"\n",
    "        feature = [1 if ch != ' ' else 0 for row in image for ch in row]\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def load_face_dataset(data_file, label_file, size=None):\n",
    "    raw_data = read_data_file(data_file)\n",
    "    raw_labels = read_labels(label_file)\n",
    "\n",
    "    features = extract_face_features(raw_data)\n",
    "    if size is not None:\n",
    "        combined = list(zip(features, raw_labels))\n",
    "        random.shuffle(combined)\n",
    "        features, raw_labels = zip(*combined[:size])\n",
    "\n",
    "    return list(features), list(raw_labels)\n",
    "\n",
    "def one_hot_encode_face(y, num_classes=2):\n",
    "    encoded = np.zeros((num_classes, len(y)))\n",
    "    for idx, val in enumerate(y):\n",
    "        encoded[val][idx] = 1\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3113e73",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06247e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d869892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    return e_Z / np.sum(e_Z, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "# Initialize weights and biases\n",
    "def initialize_parameters(input_size, hidden1_size, hidden2_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    return {\n",
    "        'W1': np.random.randn(hidden1_size, input_size) * 0.01,\n",
    "        'b1': np.zeros((hidden1_size, 1)),\n",
    "        'W2': np.random.randn(hidden2_size, hidden1_size) * 0.01,\n",
    "        'b2': np.zeros((hidden2_size, 1)),\n",
    "        'W3': np.random.randn(output_size, hidden2_size) * 0.01,\n",
    "        'b3': np.zeros((output_size, 1))\n",
    "    }\n",
    "\n",
    "# Forward pass\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    W3, b3 = parameters['W3'], parameters['b3']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    cache = (Z1, A1, Z2, A2, Z3, A3)\n",
    "    return A3, cache\n",
    "\n",
    "\n",
    "def forward_propagation_face(X, parameters, dropout_rate=0.5, training=True):\n",
    "    W1, b1 = parameters['W1'], parameters['b1']\n",
    "    W2, b2 = parameters['W2'], parameters['b2']\n",
    "    W3, b3 = parameters['W3'], parameters['b3']\n",
    "\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    if training:\n",
    "        D1 = (np.random.rand(*A1.shape) < dropout_rate).astype(float)\n",
    "        A1 *= D1\n",
    "        A1 /= dropout_rate\n",
    "    else:\n",
    "        D1 = None\n",
    "\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    if training:\n",
    "        D2 = (np.random.rand(*A2.shape) < dropout_rate).astype(float)\n",
    "        A2 *= D2\n",
    "        A2 /= dropout_rate\n",
    "    else:\n",
    "        D2 = None\n",
    "\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    # Include dropout masks in the cache\n",
    "    cache = (Z1, A1, D1, Z2, A2, D2, Z3, A3)\n",
    "    return A3, cache\n",
    "\n",
    "\n",
    "\n",
    "# Loss\n",
    "def compute_loss(Y_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "    return -np.sum(Y * np.log(Y_hat + 1e-8) + (1 - Y) * np.log(1 - Y_hat + 1e-8)) / m\n",
    "\n",
    "def compute_loss_l2(Y_hat, Y, parameters, lambda_reg=0.1):\n",
    "    m = Y.shape[1]\n",
    "    cross_entropy = -np.sum(Y * np.log(Y_hat + 1e-8)) / m\n",
    "    l2 = (lambda_reg / (2 * m)) * (\n",
    "        np.sum(np.square(parameters['W1'])) +\n",
    "        np.sum(np.square(parameters['W2'])) +\n",
    "        np.sum(np.square(parameters['W3']))\n",
    "    )\n",
    "    return cross_entropy + l2\n",
    "\n",
    "# Backward pass\n",
    "def backward_propagation(X, Y, parameters, cache):\n",
    "    m = X.shape[1]\n",
    "    W2, W3 = parameters['W2'], parameters['W3']\n",
    "    Z1, A1, Z2, A2, Z3, A3 = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW3': dW3, 'db3': db3\n",
    "    }\n",
    "\n",
    "\n",
    "def backward_propagation_face(X, Y, parameters, cache, dropout_rate=0.5):\n",
    "    m = X.shape[1]\n",
    "    W2, W3 = parameters['W2'], parameters['W3']\n",
    "    Z1, A1, D1, Z2, A2, D2, Z3, A3 = cache\n",
    "\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = (1/m) * np.dot(dZ3, A2.T)\n",
    "    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dA2 *= D2\n",
    "    dA2 /= dropout_rate\n",
    "    dZ2 = dA2 * relu_derivative(Z2)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dA1 *= D1\n",
    "    dA1 /= dropout_rate\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    return {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2,\n",
    "        'dW3': dW3, 'db3': db3\n",
    "    }\n",
    "\n",
    "# Gradient descent update\n",
    "def update_parameters(params, grads, lr):\n",
    "    for key in params:\n",
    "        params[key] -= lr * grads['d' + key]\n",
    "    return params\n",
    "\n",
    "# Prediction\n",
    "def predict_nn(X, parameters):\n",
    "    Y_hat, _ = forward_propagation(X, parameters)\n",
    "    return np.argmax(Y_hat, axis=0)\n",
    "\n",
    "def predict_nn_face(X, parameters):\n",
    "    Y_hat, _ = forward_propagation_face(X, parameters, training=False)\n",
    "    return np.argmax(Y_hat, axis=0)\n",
    "\n",
    "# Training loop\n",
    "def train_neural_net(X_train, y_train, X_test, y_test,\n",
    "                     input_size, h1, h2, output_size,\n",
    "                     epochs=1000, lr=0.1, print_loss=True,\n",
    "                     X_val=None, y_val=None, early_stopping=False, patience=10):\n",
    "    \n",
    "    parameters = initialize_parameters(input_size, h1, h2, output_size)\n",
    "    best_params = None\n",
    "    best_val_acc = 0\n",
    "    val_acc_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward and backpropagation\n",
    "        Y_hat, cache = forward_propagation(X_train, parameters)\n",
    "        loss = compute_loss(Y_hat, y_train)\n",
    "        grads = backward_propagation(X_train, y_train, parameters, cache)\n",
    "        parameters = update_parameters(parameters, grads, lr)\n",
    "\n",
    "        # Check performance every 100 epochs\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            train_preds = predict_nn(X_train, parameters)\n",
    "            train_acc = evaluate(train_preds, np.argmax(y_train, axis=0))\n",
    "            \n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_preds = predict_nn(X_val, parameters)\n",
    "                val_acc = evaluate(val_preds, np.argmax(y_val, axis=0))\n",
    "\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f} | Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "                # Save best model\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "                    val_acc_counter = 0\n",
    "                else:\n",
    "                    val_acc_counter += 1\n",
    "                    if early_stopping and val_acc_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "    final_params = best_params if best_params is not None else parameters\n",
    "\n",
    "    # Final test evaluation\n",
    "    test_preds = predict_nn(X_test, final_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return final_params\n",
    "\n",
    "\n",
    "\n",
    "def train_neural_net_face(X_train, y_train, X_test, y_test,\n",
    "                     input_size, h1, h2, output_size,\n",
    "                     epochs=1000, lr=0.1, print_loss=True,\n",
    "                     X_val=None, y_val=None, early_stopping=False,\n",
    "                     patience=10, dropout_rate=0.5, lambda_reg=0.1):  \n",
    "    \n",
    "    parameters = initialize_parameters(input_size, h1, h2, output_size)\n",
    "    best_params = None\n",
    "    best_val_acc = 0\n",
    "    val_acc_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === DROPOUT + L2 ===\n",
    "        Y_hat, cache = forward_propagation_face(X_train, parameters, dropout_rate=dropout_rate, training=True)\n",
    "        loss = compute_loss_l2(Y_hat, y_train, parameters, lambda_reg=lambda_reg)\n",
    "        grads = backward_propagation_face(X_train, y_train, parameters, cache, dropout_rate=dropout_rate)\n",
    "        parameters = update_parameters(parameters, grads, lr)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            train_preds = predict_nn_face(X_train, parameters)\n",
    "            train_acc = evaluate(train_preds, np.argmax(y_train, axis=0))\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_preds = predict_nn_face(X_val, parameters)\n",
    "                val_acc = evaluate(val_preds, np.argmax(y_val, axis=0))\n",
    "\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f} | Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    best_params = {k: v.copy() for k, v in parameters.items()}\n",
    "                    val_acc_counter = 0\n",
    "                else:\n",
    "                    val_acc_counter += 1\n",
    "                    if early_stopping and val_acc_counter >= patience:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "            else:\n",
    "                if print_loss:\n",
    "                    print(f\"Epoch {epoch}: Loss = {loss:.4f} | Train Acc = {train_acc:.4f}\")\n",
    "\n",
    "    final_params = best_params if best_params is not None else parameters\n",
    "    test_preds = predict_nn_face(X_test, final_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
    "    return final_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee0affc",
   "metadata": {},
   "source": [
    "Digit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87085246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural net on digit data\n",
      "\n",
      " DIGITS: Training on 100 samples (10%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1200 | Val Acc = 0.0900\n",
      "Epoch 100: Loss = 3.2256 | Train Acc = 0.1400 | Val Acc = 0.0960\n",
      "Epoch 200: Loss = 3.1577 | Train Acc = 0.2000 | Val Acc = 0.1460\n",
      "Epoch 300: Loss = 1.5617 | Train Acc = 0.7700 | Val Acc = 0.5240\n",
      "Epoch 400: Loss = 0.3130 | Train Acc = 1.0000 | Val Acc = 0.6300\n",
      "Epoch 500: Loss = 0.0584 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Epoch 600: Loss = 0.0249 | Train Acc = 1.0000 | Val Acc = 0.6400\n",
      "Epoch 700: Loss = 0.0146 | Train Acc = 1.0000 | Val Acc = 0.6420\n",
      "Epoch 800: Loss = 0.0099 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Epoch 900: Loss = 0.0074 | Train Acc = 1.0000 | Val Acc = 0.6400\n",
      "Epoch 999: Loss = 0.0058 | Train Acc = 1.0000 | Val Acc = 0.6380\n",
      "Final Test Accuracy: 0.5900\n",
      "DIGITS Test Accuracy with 100 samples: 0.5900\n",
      "\n",
      " DIGITS: Training on 200 samples (20%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1100 | Val Acc = 0.0840\n",
      "Epoch 100: Loss = 3.2451 | Train Acc = 0.2200 | Val Acc = 0.1360\n",
      "Epoch 200: Loss = 3.1422 | Train Acc = 0.1850 | Val Acc = 0.1620\n",
      "Epoch 300: Loss = 1.6565 | Train Acc = 0.7600 | Val Acc = 0.5780\n",
      "Epoch 400: Loss = 0.7981 | Train Acc = 0.9200 | Val Acc = 0.6480\n",
      "Epoch 500: Loss = 0.2367 | Train Acc = 0.9900 | Val Acc = 0.6920\n",
      "Epoch 600: Loss = 0.0718 | Train Acc = 1.0000 | Val Acc = 0.6960\n",
      "Epoch 700: Loss = 0.0334 | Train Acc = 1.0000 | Val Acc = 0.7000\n",
      "Epoch 800: Loss = 0.0201 | Train Acc = 1.0000 | Val Acc = 0.7020\n",
      "Epoch 900: Loss = 0.0139 | Train Acc = 1.0000 | Val Acc = 0.7040\n",
      "Epoch 999: Loss = 0.0104 | Train Acc = 1.0000 | Val Acc = 0.7000\n",
      "Final Test Accuracy: 0.6460\n",
      "DIGITS Test Accuracy with 200 samples: 0.6460\n",
      "\n",
      " DIGITS: Training on 300 samples (30%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0567 | Val Acc = 0.0500\n",
      "Epoch 100: Loss = 3.2504 | Train Acc = 0.1800 | Val Acc = 0.1240\n",
      "Epoch 200: Loss = 3.1732 | Train Acc = 0.2167 | Val Acc = 0.1400\n",
      "Epoch 300: Loss = 1.8543 | Train Acc = 0.7100 | Val Acc = 0.5620\n",
      "Epoch 400: Loss = 1.0484 | Train Acc = 0.8300 | Val Acc = 0.6700\n",
      "Epoch 500: Loss = 0.4200 | Train Acc = 0.9567 | Val Acc = 0.7240\n",
      "Epoch 600: Loss = 0.1445 | Train Acc = 1.0000 | Val Acc = 0.7240\n",
      "Epoch 700: Loss = 0.0607 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Epoch 800: Loss = 0.0335 | Train Acc = 1.0000 | Val Acc = 0.7260\n",
      "Epoch 900: Loss = 0.0218 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Epoch 999: Loss = 0.0157 | Train Acc = 1.0000 | Val Acc = 0.7280\n",
      "Final Test Accuracy: 0.6880\n",
      "DIGITS Test Accuracy with 300 samples: 0.6880\n",
      "\n",
      " DIGITS: Training on 400 samples (40%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0550 | Val Acc = 0.0540\n",
      "Epoch 100: Loss = 3.2637 | Train Acc = 0.1475 | Val Acc = 0.1160\n",
      "Epoch 200: Loss = 3.2236 | Train Acc = 0.1800 | Val Acc = 0.1540\n",
      "Epoch 300: Loss = 2.0163 | Train Acc = 0.6525 | Val Acc = 0.5820\n",
      "Epoch 400: Loss = 1.2303 | Train Acc = 0.8475 | Val Acc = 0.7100\n",
      "Epoch 500: Loss = 0.6272 | Train Acc = 0.9325 | Val Acc = 0.7640\n",
      "Epoch 600: Loss = 0.2507 | Train Acc = 0.9900 | Val Acc = 0.7440\n",
      "Epoch 700: Loss = 0.1061 | Train Acc = 1.0000 | Val Acc = 0.7420\n",
      "Epoch 800: Loss = 0.0537 | Train Acc = 1.0000 | Val Acc = 0.7480\n",
      "Epoch 900: Loss = 0.0326 | Train Acc = 1.0000 | Val Acc = 0.7560\n",
      "Epoch 999: Loss = 0.0223 | Train Acc = 1.0000 | Val Acc = 0.7600\n",
      "Final Test Accuracy: 0.6960\n",
      "DIGITS Test Accuracy with 400 samples: 0.6960\n",
      "\n",
      " DIGITS: Training on 500 samples (50%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0780 | Val Acc = 0.0960\n",
      "Epoch 100: Loss = 3.2599 | Train Acc = 0.1140 | Val Acc = 0.1100\n",
      "Epoch 200: Loss = 3.2236 | Train Acc = 0.1900 | Val Acc = 0.1580\n",
      "Epoch 300: Loss = 2.0686 | Train Acc = 0.6280 | Val Acc = 0.5540\n",
      "Epoch 400: Loss = 1.2890 | Train Acc = 0.8020 | Val Acc = 0.7020\n",
      "Epoch 500: Loss = 0.6860 | Train Acc = 0.9260 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.3405 | Train Acc = 0.9740 | Val Acc = 0.7900\n",
      "Epoch 700: Loss = 0.1629 | Train Acc = 0.9920 | Val Acc = 0.7860\n",
      "Epoch 800: Loss = 0.0816 | Train Acc = 1.0000 | Val Acc = 0.7880\n",
      "Epoch 900: Loss = 0.0468 | Train Acc = 1.0000 | Val Acc = 0.7840\n",
      "Epoch 999: Loss = 0.0308 | Train Acc = 1.0000 | Val Acc = 0.7880\n",
      "Final Test Accuracy: 0.7280\n",
      "DIGITS Test Accuracy with 500 samples: 0.7280\n",
      "\n",
      " DIGITS: Training on 600 samples (60%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.0917 | Val Acc = 0.1000\n",
      "Epoch 100: Loss = 3.2607 | Train Acc = 0.1150 | Val Acc = 0.1100\n",
      "Epoch 200: Loss = 3.2213 | Train Acc = 0.2167 | Val Acc = 0.1900\n",
      "Epoch 300: Loss = 1.9933 | Train Acc = 0.6483 | Val Acc = 0.5640\n",
      "Epoch 400: Loss = 1.2723 | Train Acc = 0.8167 | Val Acc = 0.6980\n",
      "Epoch 500: Loss = 0.7867 | Train Acc = 0.8800 | Val Acc = 0.7420\n",
      "Epoch 600: Loss = 0.4155 | Train Acc = 0.9633 | Val Acc = 0.7580\n",
      "Epoch 700: Loss = 0.2156 | Train Acc = 0.9867 | Val Acc = 0.7660\n",
      "Epoch 800: Loss = 0.1129 | Train Acc = 0.9967 | Val Acc = 0.7660\n",
      "Epoch 900: Loss = 0.0635 | Train Acc = 1.0000 | Val Acc = 0.7700\n",
      "Epoch 999: Loss = 0.0402 | Train Acc = 1.0000 | Val Acc = 0.7720\n",
      "Final Test Accuracy: 0.7600\n",
      "DIGITS Test Accuracy with 600 samples: 0.7600\n",
      "\n",
      " DIGITS: Training on 700 samples (70%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1371 | Val Acc = 0.1480\n",
      "Epoch 100: Loss = 3.2629 | Train Acc = 0.1171 | Val Acc = 0.0780\n",
      "Epoch 200: Loss = 3.2096 | Train Acc = 0.1957 | Val Acc = 0.1900\n",
      "Epoch 300: Loss = 2.0864 | Train Acc = 0.5986 | Val Acc = 0.5500\n",
      "Epoch 400: Loss = 1.2862 | Train Acc = 0.8029 | Val Acc = 0.6920\n",
      "Epoch 500: Loss = 0.8117 | Train Acc = 0.8871 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.4909 | Train Acc = 0.9457 | Val Acc = 0.7800\n",
      "Epoch 700: Loss = 0.2760 | Train Acc = 0.9786 | Val Acc = 0.7740\n",
      "Epoch 800: Loss = 0.1538 | Train Acc = 0.9914 | Val Acc = 0.7820\n",
      "Epoch 900: Loss = 0.0869 | Train Acc = 1.0000 | Val Acc = 0.7960\n",
      "Epoch 999: Loss = 0.0539 | Train Acc = 1.0000 | Val Acc = 0.8000\n",
      "Final Test Accuracy: 0.7560\n",
      "DIGITS Test Accuracy with 700 samples: 0.7560\n",
      "\n",
      " DIGITS: Training on 800 samples (80%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1363 | Val Acc = 0.1520\n",
      "Epoch 100: Loss = 3.2629 | Train Acc = 0.1163 | Val Acc = 0.0780\n",
      "Epoch 200: Loss = 3.2119 | Train Acc = 0.1913 | Val Acc = 0.1840\n",
      "Epoch 300: Loss = 2.1137 | Train Acc = 0.5962 | Val Acc = 0.5520\n",
      "Epoch 400: Loss = 1.2931 | Train Acc = 0.8013 | Val Acc = 0.7040\n",
      "Epoch 500: Loss = 0.8307 | Train Acc = 0.8838 | Val Acc = 0.7700\n",
      "Epoch 600: Loss = 0.5243 | Train Acc = 0.9375 | Val Acc = 0.7920\n",
      "Epoch 700: Loss = 0.3128 | Train Acc = 0.9738 | Val Acc = 0.8000\n",
      "Epoch 800: Loss = 0.1811 | Train Acc = 0.9888 | Val Acc = 0.8080\n",
      "Epoch 900: Loss = 0.1049 | Train Acc = 0.9975 | Val Acc = 0.8100\n",
      "Epoch 999: Loss = 0.0641 | Train Acc = 1.0000 | Val Acc = 0.8180\n",
      "Final Test Accuracy: 0.7640\n",
      "DIGITS Test Accuracy with 800 samples: 0.7640\n",
      "\n",
      " DIGITS: Training on 900 samples (90%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1178 | Val Acc = 0.1380\n",
      "Epoch 100: Loss = 3.2622 | Train Acc = 0.1122 | Val Acc = 0.0800\n",
      "Epoch 200: Loss = 3.2113 | Train Acc = 0.2300 | Val Acc = 0.2060\n",
      "Epoch 300: Loss = 2.0194 | Train Acc = 0.5889 | Val Acc = 0.5340\n",
      "Epoch 400: Loss = 1.2823 | Train Acc = 0.7956 | Val Acc = 0.7060\n",
      "Epoch 500: Loss = 0.8499 | Train Acc = 0.8767 | Val Acc = 0.7640\n",
      "Epoch 600: Loss = 0.5574 | Train Acc = 0.9333 | Val Acc = 0.8000\n",
      "Epoch 700: Loss = 0.3543 | Train Acc = 0.9656 | Val Acc = 0.8080\n",
      "Epoch 800: Loss = 0.2110 | Train Acc = 0.9867 | Val Acc = 0.8180\n",
      "Epoch 900: Loss = 0.1278 | Train Acc = 0.9944 | Val Acc = 0.8220\n",
      "Epoch 999: Loss = 0.0794 | Train Acc = 1.0000 | Val Acc = 0.8280\n",
      "Final Test Accuracy: 0.7740\n",
      "DIGITS Test Accuracy with 900 samples: 0.7740\n",
      "\n",
      " DIGITS: Training on 1000 samples (100%)\n",
      "Epoch 0: Loss = 6.9313 | Train Acc = 0.1150 | Val Acc = 0.1300\n",
      "Epoch 100: Loss = 3.2654 | Train Acc = 0.1120 | Val Acc = 0.1000\n",
      "Epoch 200: Loss = 3.2206 | Train Acc = 0.2240 | Val Acc = 0.2000\n",
      "Epoch 300: Loss = 2.0562 | Train Acc = 0.5750 | Val Acc = 0.5260\n",
      "Epoch 400: Loss = 1.3209 | Train Acc = 0.7800 | Val Acc = 0.6980\n",
      "Epoch 500: Loss = 0.8731 | Train Acc = 0.8640 | Val Acc = 0.7820\n",
      "Epoch 600: Loss = 0.5899 | Train Acc = 0.9220 | Val Acc = 0.8140\n",
      "Epoch 700: Loss = 0.3715 | Train Acc = 0.9590 | Val Acc = 0.8200\n",
      "Epoch 800: Loss = 0.2315 | Train Acc = 0.9820 | Val Acc = 0.8360\n",
      "Epoch 900: Loss = 0.1428 | Train Acc = 0.9940 | Val Acc = 0.8400\n",
      "Epoch 999: Loss = 0.0914 | Train Acc = 0.9970 | Val Acc = 0.8400\n",
      "Final Test Accuracy: 0.7940\n",
      "DIGITS Test Accuracy with 1000 samples: 0.7940\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing neural net on digit data\")\n",
    "\n",
    "X_train_raw, y_train_raw = load_dataset(train_data_file, train_label_file, size=NUM_TRAINING)\n",
    "X_val_raw, y_val_raw     = load_dataset(val_data_file, val_label_file, size=NUM_VALIDATION)\n",
    "X_test_raw, y_test_raw   = load_dataset(test_data_file, test_label_file, size=NUM_TESTING)\n",
    "\n",
    "X_train = np.array(X_train_raw).T\n",
    "X_val   = np.array(X_val_raw).T\n",
    "X_test  = np.array(X_test_raw).T\n",
    "\n",
    "y_train = one_hot_encode(y_train_raw)\n",
    "y_val   = one_hot_encode(y_val_raw)\n",
    "y_test  = one_hot_encode(y_test_raw)\n",
    "\n",
    "# Train on increasing percentages of DIGIT data \n",
    "percentages = [0.1 * i for i in range(1, 11)]  # 10% to 100%\n",
    "total_digit_samples = X_train.shape[1]\n",
    "\n",
    "digit_results = []\n",
    "\n",
    "for pct in percentages:\n",
    "    n = int(pct * total_digit_samples)    \n",
    "    X_subset = X_train[:, :n]\n",
    "    y_subset = y_train[:, :n]\n",
    "\n",
    "    print(f\"\\n DIGITS: Training on {n} samples ({int(pct * 100)}%)\")\n",
    "    \n",
    "    trained_params = train_neural_net(\n",
    "        X_subset, y_subset,\n",
    "        X_test, y_test,\n",
    "        input_size=784, h1=128, h2=64, output_size=10,\n",
    "        epochs=1000, lr=0.1,\n",
    "        X_val=X_val, y_val=y_val,\n",
    "        early_stopping=True, patience=10\n",
    "    )\n",
    "\n",
    "    test_preds = predict_nn(X_test, trained_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_test, axis=0))\n",
    "    digit_results.append((n, test_acc))\n",
    "    print(f\"DIGITS Test Accuracy with {n} samples: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef98d09",
   "metadata": {},
   "source": [
    "Face data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c5dfbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing neural net on face data\n",
      "\n",
      " FACES: Training on 45 samples (10%)\n",
      "Epoch 0: Loss = 0.7682 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7618 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7194 | Train Acc = 0.5556 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.1878 | Train Acc = 1.0000 | Val Acc = 0.7143\n",
      "Epoch 400: Loss = 0.1487 | Train Acc = 1.0000 | Val Acc = 0.6910\n",
      "Epoch 500: Loss = 0.1312 | Train Acc = 1.0000 | Val Acc = 0.6146\n",
      "Epoch 600: Loss = 0.1297 | Train Acc = 1.0000 | Val Acc = 0.6478\n",
      "Epoch 700: Loss = 0.1343 | Train Acc = 1.0000 | Val Acc = 0.6744\n",
      "Epoch 800: Loss = 0.1378 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Epoch 900: Loss = 0.1354 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Epoch 999: Loss = 0.1407 | Train Acc = 1.0000 | Val Acc = 0.6512\n",
      "Final Test Accuracy: 0.6733\n",
      "FACES Test Accuracy with 45 samples: 0.6733\n",
      "\n",
      " FACES: Training on 90 samples (20%)\n",
      "Epoch 0: Loss = 0.7307 | Train Acc = 0.5000 | Val Acc = 0.4751\n",
      "Epoch 100: Loss = 0.7306 | Train Acc = 0.7444 | Val Acc = 0.5415\n",
      "Epoch 200: Loss = 0.7269 | Train Acc = 0.9556 | Val Acc = 0.6113\n",
      "Epoch 300: Loss = 0.1093 | Train Acc = 1.0000 | Val Acc = 0.7409\n",
      "Epoch 400: Loss = 0.0938 | Train Acc = 1.0000 | Val Acc = 0.7409\n",
      "Epoch 500: Loss = 0.0881 | Train Acc = 1.0000 | Val Acc = 0.7243\n",
      "Epoch 600: Loss = 0.0842 | Train Acc = 1.0000 | Val Acc = 0.7243\n",
      "Epoch 700: Loss = 0.0766 | Train Acc = 1.0000 | Val Acc = 0.6910\n",
      "Epoch 800: Loss = 0.0827 | Train Acc = 1.0000 | Val Acc = 0.6844\n",
      "Epoch 900: Loss = 0.1012 | Train Acc = 1.0000 | Val Acc = 0.6944\n",
      "Epoch 999: Loss = 0.0807 | Train Acc = 1.0000 | Val Acc = 0.6611\n",
      "Final Test Accuracy: 0.6933\n",
      "FACES Test Accuracy with 90 samples: 0.6933\n",
      "\n",
      " FACES: Training on 135 samples (30%)\n",
      "Epoch 0: Loss = 0.7182 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7168 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7164 | Train Acc = 0.5259 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6078 | Train Acc = 0.9481 | Val Acc = 0.5714\n",
      "Epoch 400: Loss = 0.1291 | Train Acc = 1.0000 | Val Acc = 0.7907\n",
      "Epoch 500: Loss = 0.0879 | Train Acc = 1.0000 | Val Acc = 0.7708\n",
      "Epoch 600: Loss = 0.0609 | Train Acc = 1.0000 | Val Acc = 0.7807\n",
      "Epoch 700: Loss = 0.0706 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Epoch 800: Loss = 0.0621 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Epoch 900: Loss = 0.0748 | Train Acc = 1.0000 | Val Acc = 0.7641\n",
      "Epoch 999: Loss = 0.0590 | Train Acc = 1.0000 | Val Acc = 0.7575\n",
      "Final Test Accuracy: 0.7467\n",
      "FACES Test Accuracy with 135 samples: 0.7467\n",
      "\n",
      " FACES: Training on 180 samples (40%)\n",
      "Epoch 0: Loss = 0.7119 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7112 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7109 | Train Acc = 0.5167 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.5626 | Train Acc = 0.9778 | Val Acc = 0.7110\n",
      "Epoch 400: Loss = 0.1048 | Train Acc = 1.0000 | Val Acc = 0.8272\n",
      "Epoch 500: Loss = 0.0661 | Train Acc = 1.0000 | Val Acc = 0.8073\n",
      "Epoch 600: Loss = 0.0570 | Train Acc = 1.0000 | Val Acc = 0.8106\n",
      "Epoch 700: Loss = 0.0480 | Train Acc = 1.0000 | Val Acc = 0.8007\n",
      "Epoch 800: Loss = 0.0408 | Train Acc = 1.0000 | Val Acc = 0.7973\n",
      "Epoch 900: Loss = 0.0464 | Train Acc = 1.0000 | Val Acc = 0.7907\n",
      "Epoch 999: Loss = 0.0453 | Train Acc = 1.0000 | Val Acc = 0.7741\n",
      "Final Test Accuracy: 0.8067\n",
      "FACES Test Accuracy with 180 samples: 0.8067\n",
      "\n",
      " FACES: Training on 225 samples (50%)\n",
      "Epoch 0: Loss = 0.7082 | Train Acc = 0.5156 | Val Acc = 0.5249\n",
      "Epoch 100: Loss = 0.7081 | Train Acc = 0.5022 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7079 | Train Acc = 0.5022 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6985 | Train Acc = 0.8978 | Val Acc = 0.6811\n",
      "Epoch 400: Loss = 0.1540 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 500: Loss = 0.0799 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 600: Loss = 0.0544 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 700: Loss = 0.0367 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 800: Loss = 0.0459 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 900: Loss = 0.0441 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 999: Loss = 0.0413 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Final Test Accuracy: 0.8800\n",
      "FACES Test Accuracy with 225 samples: 0.8800\n",
      "\n",
      " FACES: Training on 270 samples (60%)\n",
      "Epoch 0: Loss = 0.7056 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7039 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7037 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7012 | Train Acc = 0.5296 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.2080 | Train Acc = 1.0000 | Val Acc = 0.8571\n",
      "Epoch 500: Loss = 0.0865 | Train Acc = 1.0000 | Val Acc = 0.8671\n",
      "Epoch 600: Loss = 0.0503 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 700: Loss = 0.0451 | Train Acc = 1.0000 | Val Acc = 0.8605\n",
      "Epoch 800: Loss = 0.0401 | Train Acc = 1.0000 | Val Acc = 0.8439\n",
      "Epoch 900: Loss = 0.0376 | Train Acc = 1.0000 | Val Acc = 0.8505\n",
      "Epoch 999: Loss = 0.0388 | Train Acc = 1.0000 | Val Acc = 0.8472\n",
      "Final Test Accuracy: 0.8733\n",
      "FACES Test Accuracy with 270 samples: 0.8733\n",
      "\n",
      " FACES: Training on 315 samples (70%)\n",
      "Epoch 0: Loss = 0.7038 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7032 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7031 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7008 | Train Acc = 0.5175 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.2038 | Train Acc = 1.0000 | Val Acc = 0.8771\n",
      "Epoch 500: Loss = 0.0844 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 600: Loss = 0.0550 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 700: Loss = 0.0422 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 800: Loss = 0.0383 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 900: Loss = 0.0428 | Train Acc = 1.0000 | Val Acc = 0.8538\n",
      "Epoch 999: Loss = 0.0364 | Train Acc = 1.0000 | Val Acc = 0.8472\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 315 samples: 0.8600\n",
      "\n",
      " FACES: Training on 360 samples (80%)\n",
      "Epoch 0: Loss = 0.7025 | Train Acc = 0.4972 | Val Acc = 0.5150\n",
      "Epoch 100: Loss = 0.7025 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7023 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.7011 | Train Acc = 0.5056 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.4001 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 500: Loss = 0.1075 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Epoch 600: Loss = 0.0500 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 700: Loss = 0.0462 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 800: Loss = 0.0536 | Train Acc = 1.0000 | Val Acc = 0.8738\n",
      "Epoch 900: Loss = 0.0498 | Train Acc = 1.0000 | Val Acc = 0.8571\n",
      "Epoch 999: Loss = 0.0364 | Train Acc = 1.0000 | Val Acc = 0.8605\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 360 samples: 0.8600\n",
      "\n",
      " FACES: Training on 405 samples (90%)\n",
      "Epoch 0: Loss = 0.7015 | Train Acc = 0.5012 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.7014 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.7012 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6999 | Train Acc = 0.5062 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.3557 | Train Acc = 1.0000 | Val Acc = 0.8804\n",
      "Epoch 500: Loss = 0.0966 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 600: Loss = 0.0605 | Train Acc = 1.0000 | Val Acc = 0.8937\n",
      "Epoch 700: Loss = 0.0347 | Train Acc = 1.0000 | Val Acc = 0.8937\n",
      "Epoch 800: Loss = 0.0318 | Train Acc = 1.0000 | Val Acc = 0.8904\n",
      "Epoch 900: Loss = 0.0311 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 999: Loss = 0.0267 | Train Acc = 1.0000 | Val Acc = 0.8837\n",
      "Final Test Accuracy: 0.8667\n",
      "FACES Test Accuracy with 405 samples: 0.8667\n",
      "\n",
      " FACES: Training on 451 samples (100%)\n",
      "Epoch 0: Loss = 0.7006 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 100: Loss = 0.6999 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 200: Loss = 0.6999 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 300: Loss = 0.6991 | Train Acc = 0.5188 | Val Acc = 0.5183\n",
      "Epoch 400: Loss = 0.6038 | Train Acc = 0.9534 | Val Acc = 0.8073\n",
      "Epoch 500: Loss = 0.1134 | Train Acc = 1.0000 | Val Acc = 0.8870\n",
      "Epoch 600: Loss = 0.0777 | Train Acc = 1.0000 | Val Acc = 0.8738\n",
      "Epoch 700: Loss = 0.0492 | Train Acc = 1.0000 | Val Acc = 0.8704\n",
      "Epoch 800: Loss = 0.0495 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 900: Loss = 0.0279 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Epoch 999: Loss = 0.0299 | Train Acc = 1.0000 | Val Acc = 0.8638\n",
      "Final Test Accuracy: 0.8600\n",
      "FACES Test Accuracy with 451 samples: 0.8600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Testing neural net on face data\")\n",
    "\n",
    "# Load and process face data\n",
    "X_face_train_raw, y_face_train_raw = load_face_dataset(face_train_data_file, face_train_label_file, size=NUM_FACE_TRAINING)\n",
    "X_face_val_raw, y_face_val_raw     = load_face_dataset(face_val_data_file, face_val_label_file, size=NUM_FACE_VALIDATION)\n",
    "X_face_test_raw, y_face_test_raw   = load_face_dataset(face_test_data_file, face_test_label_file, size=NUM_FACE_TESTING)\n",
    "\n",
    "X_face_train = np.array(X_face_train_raw).T\n",
    "X_face_val   = np.array(X_face_val_raw).T\n",
    "X_face_test  = np.array(X_face_test_raw).T\n",
    "\n",
    "y_face_train = one_hot_encode_face(y_face_train_raw)\n",
    "y_face_val   = one_hot_encode_face(y_face_val_raw)\n",
    "y_face_test  = one_hot_encode_face(y_face_test_raw)\n",
    "\n",
    "# Train on increasing percentages of FACE data \n",
    "percentages = [0.1 * i for i in range(1, 11)]  # 10% to 100%\n",
    "total_face_samples = X_face_train.shape[1]\n",
    "\n",
    "face_results = []\n",
    "\n",
    "for pct in percentages:\n",
    "    n = int(pct * total_face_samples)\n",
    "\n",
    "    X_subset = X_face_train[:, :n]\n",
    "    y_subset = y_face_train[:, :n]\n",
    "\n",
    "    print(f\"\\n FACES: Training on {n} samples ({int(pct * 100)}%)\")\n",
    "\n",
    "    trained_params = train_neural_net_face(\n",
    "        X_subset, y_subset,\n",
    "        X_face_test, y_face_test,\n",
    "        input_size=4200, h1=32, h2=16, output_size=2,\n",
    "        epochs=1000, lr=0.1,\n",
    "        X_val=X_face_val, y_val=y_face_val,\n",
    "        early_stopping=True, patience=10,\n",
    "        dropout_rate=0.5,  # dropout to decrease overfitting   \n",
    "        lambda_reg=0.5  #l2 regularization to decrease overfitting\n",
    "    )\n",
    "\n",
    "\n",
    "    test_preds = predict_nn_face(X_face_test, trained_params)\n",
    "    test_acc = evaluate(test_preds, np.argmax(y_face_test, axis=0))\n",
    "    face_results.append((n, test_acc))\n",
    "    print(f\"FACES Test Accuracy with {n} samples: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3351685d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
